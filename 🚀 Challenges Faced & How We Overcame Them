ğŸš€ Challenges Faced & How We Overcame Them
ğŸ”´ Overfitting on MNIST
Challenge: The model performed well on the training set but showed slightly lower accuracy on test data.
âœ… Solution: We implemented dropout layers, L2 regularization, and early stopping to prevent overfitting and improve generalization.

â³ Computational Inefficiency
Challenge: Training took longer due to the large dataset and multiple convolutional layers.
âš¡ Solution: We used Google Colabâ€™s GPU acceleration and optimized the model by reducing redundant layers, applying batch normalization, and using smaller kernel sizes to speed up training.

âœï¸ Limited Generalization on Custom Handwritten Images
Challenge: The model struggled to recognize variations in handwriting styles different from the training dataset.
ğŸ” Solution: We expanded the dataset using data augmentation (rotation, scaling, shearing, and contrast adjustments) to make the model more robust to handwriting variations.

ğŸ“‰ Low Accuracy on Noisy Images
Challenge: Slight distortions, blur, and noise in handwritten digits led to frequent misclassifications.
ğŸ¯ Solution: We applied preprocessing techniques such as Gaussian blurring, adaptive thresholding, and noise reduction to improve image quality before feeding them into the model.
